{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e157f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d5f5a",
   "metadata": {},
   "source": [
    "# autograd & optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c38e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Module\n",
    "\n",
    "# input / output / Forward / Backward\n",
    "\n",
    "# 학습의 대상이되는 parameter(tensor)정의\n",
    "\n",
    "# nn.Parameter\n",
    "# - tensor의 상속객체 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5882bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a5345bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear (nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        return x @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2da574f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3896,  1.1459, -0.0348, -1.5924,  0.4654, -0.1913, -0.5877],\n",
       "        [ 0.3763,  0.6942, -1.8856, -1.8911, -1.2746, -1.9195, -1.3673],\n",
       "        [ 0.9060,  1.0741, -2.2453, -1.3985, -0.2711,  0.7508, -0.3743],\n",
       "        [ 0.7015, -0.4305, -0.8190,  1.5281, -0.7773, -0.4323,  1.2546],\n",
       "        [ 1.0900, -0.4190,  0.0659, -1.5090, -1.5751,  0.3330,  0.0050]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5,7)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6c4f75bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3379, -4.3404,  4.3791],\n",
       "        [-4.5275, -0.9921,  3.7437],\n",
       "        [-2.5923, -1.3030,  0.6212],\n",
       "        [ 0.7383,  1.2610, -2.4558],\n",
       "        [ 0.4386, -3.1067, -0.5780]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = MyLinear(7,3)\n",
    "a.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "db53811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = nn.Parameter(torch.randn(in_features,out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "    \n",
    "    def forward(self, x:Tensor):\n",
    "        x = x @ self.weights + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3f233b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9874,  5.6381,  0.2759,  1.9646, -3.6695],\n",
       "        [-2.7893, -3.0407,  2.5097, -3.7958,  0.5242],\n",
       "        [ 0.1104,  0.9346,  0.7894, -0.5881, -0.5484],\n",
       "        [ 2.1674,  3.5793,  0.9341, -1.0308, -4.6447],\n",
       "        [-1.3322,  0.5629,  0.7290,  0.7674,  1.5763],\n",
       "        [ 0.6718,  0.2515, -0.1364,  0.0555,  1.3575],\n",
       "        [ 2.4978,  1.3639, -0.4062, -0.5186, -0.7460]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = LinearLayer(3,5)\n",
    "a.weights, a.bias\n",
    "\n",
    "x = torch.randn(7,3)\n",
    "a.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddaae19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27390c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b513c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1,1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2ebfa2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.]], dtype=float32),\n",
       " array([[ 1.],\n",
       "        [ 3.],\n",
       "        [ 5.],\n",
       "        [ 7.],\n",
       "        [ 9.],\n",
       "        [11.],\n",
       "        [13.],\n",
       "        [15.],\n",
       "        [17.],\n",
       "        [19.],\n",
       "        [21.]], dtype=float32))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "172afb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self,inputSize,outputSize):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ea731e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1\n",
    "outputDim = 1\n",
    "learningRate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "model = LinearRegression(inputDim, outputDim)\n",
    "\n",
    "# cuda \n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f4321f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss func and optim \n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "38dd060f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.]], device='cuda:0')"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input example\n",
    "Variable(torch.from_numpy(x_train).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a7abc4bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0490, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 0, loss: 0.0489596463739872\n",
      "tensor(0.0484, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 1, loss: 0.04841294884681702\n",
      "tensor(0.0479, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 2, loss: 0.04787225276231766\n",
      "tensor(0.0473, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 3, loss: 0.047337744385004044\n",
      "tensor(0.0468, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 4, loss: 0.04680907353758812\n",
      "tensor(0.0463, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 5, loss: 0.046286385506391525\n",
      "tensor(0.0458, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 6, loss: 0.04576951265335083\n",
      "tensor(0.0453, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 7, loss: 0.045258451253175735\n",
      "tensor(0.0448, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 8, loss: 0.04475307837128639\n",
      "tensor(0.0443, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 9, loss: 0.04425327852368355\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 10, loss: 0.043759096413850784\n",
      "tensor(0.0433, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 11, loss: 0.043270450085401535\n",
      "tensor(0.0428, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 12, loss: 0.04278724640607834\n",
      "tensor(0.0423, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 13, loss: 0.04230942204594612\n",
      "tensor(0.0418, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 14, loss: 0.04183700308203697\n",
      "tensor(0.0414, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 15, loss: 0.04136985167860985\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 16, loss: 0.04090786352753639\n",
      "tensor(0.0405, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 17, loss: 0.04045102745294571\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 18, loss: 0.03999935835599899\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 19, loss: 0.03955268859863281\n",
      "tensor(0.0391, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 20, loss: 0.039111003279685974\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 21, loss: 0.03867417201399803\n",
      "tensor(0.0382, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 22, loss: 0.03824234753847122\n",
      "tensor(0.0378, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 23, loss: 0.037815339863300323\n",
      "tensor(0.0374, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 24, loss: 0.037393055856227875\n",
      "tensor(0.0370, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 25, loss: 0.03697545826435089\n",
      "tensor(0.0366, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 26, loss: 0.03656256943941116\n",
      "tensor(0.0362, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 27, loss: 0.036154281347990036\n",
      "tensor(0.0358, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 28, loss: 0.03575058653950691\n",
      "tensor(0.0354, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 29, loss: 0.03535132110118866\n",
      "tensor(0.0350, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 30, loss: 0.034956544637680054\n",
      "tensor(0.0346, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 31, loss: 0.03456622362136841\n",
      "tensor(0.0342, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 32, loss: 0.03418022394180298\n",
      "tensor(0.0338, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 33, loss: 0.03379855304956436\n",
      "tensor(0.0334, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 34, loss: 0.0334211066365242\n",
      "tensor(0.0330, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 35, loss: 0.03304795175790787\n",
      "tensor(0.0327, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 36, loss: 0.03267887234687805\n",
      "tensor(0.0323, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 37, loss: 0.03231395035982132\n",
      "tensor(0.0320, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 38, loss: 0.031953126192092896\n",
      "tensor(0.0316, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 39, loss: 0.03159632161259651\n",
      "tensor(0.0312, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 40, loss: 0.03124346397817135\n",
      "tensor(0.0309, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 41, loss: 0.030894586816430092\n",
      "tensor(0.0305, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 42, loss: 0.030549578368663788\n",
      "tensor(0.0302, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 43, loss: 0.03020845353603363\n",
      "tensor(0.0299, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 44, loss: 0.02987109124660492\n",
      "tensor(0.0295, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 45, loss: 0.02953755110502243\n",
      "tensor(0.0292, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 46, loss: 0.029207706451416016\n",
      "tensor(0.0289, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 47, loss: 0.028881577774882317\n",
      "tensor(0.0286, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 48, loss: 0.028559014201164246\n",
      "tensor(0.0282, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 49, loss: 0.028240106999874115\n",
      "tensor(0.0279, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 50, loss: 0.027924761176109314\n",
      "tensor(0.0276, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 51, loss: 0.0276129599660635\n",
      "tensor(0.0273, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 52, loss: 0.02730458788573742\n",
      "tensor(0.0270, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 53, loss: 0.026999622583389282\n",
      "tensor(0.0267, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 54, loss: 0.026698175817728043\n",
      "tensor(0.0264, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 55, loss: 0.026400046423077583\n",
      "tensor(0.0261, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 56, loss: 0.026105236262083054\n",
      "tensor(0.0258, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 57, loss: 0.025813745334744453\n",
      "tensor(0.0255, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 58, loss: 0.025525471195578575\n",
      "tensor(0.0252, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 59, loss: 0.02524041384458542\n",
      "tensor(0.0250, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 60, loss: 0.02495863102376461\n",
      "tensor(0.0247, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 61, loss: 0.024679863825440407\n",
      "tensor(0.0244, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 62, loss: 0.024404287338256836\n",
      "tensor(0.0241, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 63, loss: 0.024131804704666138\n",
      "tensor(0.0239, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 64, loss: 0.02386230044066906\n",
      "tensor(0.0236, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 65, loss: 0.023595845326781273\n",
      "tensor(0.0233, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 66, loss: 0.02333233319222927\n",
      "tensor(0.0231, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 67, loss: 0.023071736097335815\n",
      "tensor(0.0228, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 68, loss: 0.022814124822616577\n",
      "tensor(0.0226, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 69, loss: 0.022559382021427155\n",
      "tensor(0.0223, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 70, loss: 0.02230745553970337\n",
      "tensor(0.0221, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 71, loss: 0.022058337926864624\n",
      "tensor(0.0218, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 72, loss: 0.02181205153465271\n",
      "tensor(0.0216, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 73, loss: 0.021568462252616882\n",
      "tensor(0.0213, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 74, loss: 0.02132764458656311\n",
      "tensor(0.0211, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 75, loss: 0.021089473739266396\n",
      "tensor(0.0209, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 76, loss: 0.02085394598543644\n",
      "tensor(0.0206, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 77, loss: 0.020621109753847122\n",
      "tensor(0.0204, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 78, loss: 0.020390817895531654\n",
      "tensor(0.0202, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 79, loss: 0.020163124427199364\n",
      "tensor(0.0199, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 80, loss: 0.019937943667173386\n",
      "tensor(0.0197, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 81, loss: 0.019715288653969765\n",
      "tensor(0.0195, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 82, loss: 0.019495157524943352\n",
      "tensor(0.0193, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 83, loss: 0.01927744597196579\n",
      "tensor(0.0191, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 84, loss: 0.019062194973230362\n",
      "tensor(0.0188, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 85, loss: 0.018849294632673264\n",
      "tensor(0.0186, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 86, loss: 0.018638836219906807\n",
      "tensor(0.0184, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 87, loss: 0.018430689349770546\n",
      "tensor(0.0182, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 88, loss: 0.018224874511361122\n",
      "tensor(0.0180, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 89, loss: 0.01802138052880764\n",
      "tensor(0.0178, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 90, loss: 0.017820164561271667\n",
      "tensor(0.0176, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 91, loss: 0.017621131613850594\n",
      "tensor(0.0174, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 92, loss: 0.017424339428544044\n",
      "tensor(0.0172, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 93, loss: 0.017229752615094185\n",
      "tensor(0.0170, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 94, loss: 0.01703738421201706\n",
      "tensor(0.0168, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 95, loss: 0.01684715412557125\n",
      "tensor(0.0167, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 96, loss: 0.016659025102853775\n",
      "tensor(0.0165, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 97, loss: 0.016472963616251945\n",
      "tensor(0.0163, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 98, loss: 0.016289010643959045\n",
      "tensor(0.0161, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch: 99, loss: 0.016107114031910896\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "    labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    \n",
    "    # each epoch , gradient 초기화 \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward : get output from the model, given the input \n",
    "    # model = LinearRegression(inputDim, outputDim)\n",
    "    outputs = model(inputs) # y-hat\n",
    "    \n",
    "    # calculate MSE loss \n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    \n",
    "    # get gradient w, r, t to parameters \n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    # update parameters \n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"epoch: {}, loss: {}\".format(epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e170c4f",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ed167a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, dim, lr = torch.scalar_tensor(0.01)):\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        \n",
    "        self.weights = torch.zeros(dim, 1, dtype = torch.float).to(device)\n",
    "        self.bias = torch.scalar_tensor(0).to(device)\n",
    "        self.grads = {\"dw\": torch.zeros(dim,1,dtype=torch.float).to(device),\n",
    "                     \"db\": torch.scalar_tensor(0).to(device)}\n",
    "        self.lr = lr.to(device)\n",
    "    \n",
    "    # sigmoid 정의 \n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1+torch.exp(-z))\n",
    "    \n",
    "    # forward\n",
    "    def forward(self,x):\n",
    "        z = torch.mm(self.weights.T,x)\n",
    "        a = self.sigmoid(z)\n",
    "        return a \n",
    "    \n",
    "    def backward(self,x,yhat,y):\n",
    "        self.grads[\"dw\"] = (1/x.shape[1]) * torch.mm(x,(yhat-y).T)\n",
    "        self.grads['db'] = (1/x.shape[1]) * torch.sum(yhat-y)\n",
    "        \n",
    "    def optimize(self):\n",
    "        self.weights = self.weights - self.lr* self.grads['dw']\n",
    "        self.bias = self.bias - self.lr * self.grads['db']\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "# loss function    \n",
    "def loss(yhat, y):\n",
    "    m = y.size()[1]\n",
    "    return -(1/m) * torch.sum(y*torch.log(yhat) + (1-y) * torch.log(1-yhat))\n",
    "\n",
    "# prediction \n",
    "def predict(yhat, y):\n",
    "    y_pred = torch.zeros(1, y.size()[1])\n",
    "    for i in range(yhat.size()[1]):\n",
    "        if yhat[0,i] <= 0.5:\n",
    "            y_pred[0,i] = 0\n",
    "        else:\n",
    "            y_pred[0,i] = 1\n",
    "    return 100 - torch.mean(torch.abs(y_pred - y)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d8644ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(3)\n",
    "model.to(device)\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "83f662f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0100)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.scalar_tensor(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5dce3113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1712],\n",
       "        [4.3063],\n",
       "        [0.1323]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(torch.randn(3,3), torch.randn(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "74a70a59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.zeros(3,1,dtype=torch.float)\n",
    "z.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc50c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
